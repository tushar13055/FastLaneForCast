{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formula 1 Grand Prix result prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This project is aimed towards predicting the future F1 GP winners based on the drivers, constructors or both \n",
    "### Things to keep in mind\n",
    "\n",
    "Before begining the project we need to understand the history of F1 and the diffrent eras in which a certain driver or constructor dominated the whole grid. Here are some important eras of F1 after 2010.  \n",
    "\n",
    "* 1994-2009 Schumacher (Scuderia Ferrari)\n",
    "* 2007-2010 Alonso (Renault,Scuderia Ferrari)\n",
    "* 2011-2013 Vettle (Redbull Racing)\n",
    "* 2014-Present Hamilton (Mercedes-Benz)\n",
    "\n",
    "F1 Constructors performance are largely dependent on the FIA techinical regulation for the season after the 2013 season new engine regulation were made (Hybrid era) Mercedes-Benz are most dominat team since followed bu Redbull Racing and Scuderia Ferrari. Rules are set to change for 2022 so whatever analysis made here will not apply for 2022 season and so far. only data after 2010 will be considered in the following analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunbeam/.local/lib/python3.6/site-packages/pyspark/context.py:238: FutureWarning: Python 3.6 support is deprecated in Spark 3.2.\n",
      "  FutureWarning\n"
     ]
    }
   ],
   "source": [
    "#Entrypoint 2.x\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "spark = SparkSession.builder.config(\"spark.sql.shuffle.partitions\", \"2\").appName(\"InjestionProcessing\").master(\"local[2]\").getOrCreate()\n",
    "\n",
    "# On yarn:\n",
    "# spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().master(\"yarn\").getOrCreate()\n",
    "# specify .master(\"yarn\")\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hdfs://localhost:9000/user/sunbeam/f1/data/'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%run \"includes/configuration\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = spark.read.option(\"header\", True).csv(f\"{data}/results.csv\")\n",
    "races = spark.read.option(\"header\", True).csv(f\"{data}/races.csv\")\n",
    "qualifying = spark.read.option(\"header\", True).csv(f\"{data}/qualifying.csv\")\n",
    "drivers = spark.read.option(\"header\", True).csv(f\"{data}/drivers.csv\")\n",
    "constructors = spark.read.option(\"header\", True).csv(f\"{data}/constructors.csv\")\n",
    "circuits = spark.read.option(\"header\", True).csv(f\"{data}/circuits.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename columns for better understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['resultId',\n",
       " 'raceId',\n",
       " 'driverId',\n",
       " 'constructorId',\n",
       " 'number',\n",
       " 'grid',\n",
       " 'position',\n",
       " 'positionText',\n",
       " 'positionOrder',\n",
       " 'points',\n",
       " 'laps',\n",
       " 'time',\n",
       " 'milliseconds',\n",
       " 'fastestLap',\n",
       " 'rank',\n",
       " 'fastestLapTime',\n",
       " 'fastestLapSpeed',\n",
       " 'statusId']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.withColumnRenamed(\"resultId\", \"result_id\").withColumnRenamed(\"raceId\", \"race_id\").withColumnRenamed(\"constructorId\", \"constructor_id\").withColumnRenamed(\"statusId\", \"status_id\").withColumnRenamed(\"number\", \"results_number\").withColumnRenamed(\"time\", \"results_time\").withColumnRenamed(\"driverId\", \"driver_id\").withColumnRenamed(\"position\", \"result_position\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['raceId', 'year', 'round', 'circuitId', 'name', 'date', 'time', 'url']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "races.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "races = races.withColumnRenamed(\"raceId\", \"race_id\").withColumnRenamed(\"circuitId\", \"circuit_id\").withColumnRenamed(\"url\", \"race_url\").withColumnRenamed(\"time\", \"race_time\").withColumnRenamed(\"name\", \"race_name\").withColumnRenamed(\"raceId\", \"race_id\").withColumnRenamed(\"circuitId\", \"circuit_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['qualifyId',\n",
       " 'raceId',\n",
       " 'driverId',\n",
       " 'constructorId',\n",
       " 'number',\n",
       " 'position',\n",
       " 'q1',\n",
       " 'q2',\n",
       " 'q3']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qualifying.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualifying = qualifying.withColumnRenamed(\"number\", \"qualifying_number\").withColumnRenamed(\"qualifyingId\", \"qualifying_id\").withColumnRenamed(\"raceId\", \"race_id\").withColumnRenamed(\"driverId\", \"driver_id\").withColumnRenamed(\"constructorId\", \"constructor_id\").withColumnRenamed(\"position\", \"qualifying_position\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['driverId',\n",
       " 'driverRef',\n",
       " 'number',\n",
       " 'code',\n",
       " 'forename',\n",
       " 'surname',\n",
       " 'dob',\n",
       " 'nationality',\n",
       " 'url']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drivers.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "drivers = drivers.withColumnRenamed(\"number\", \"driver_number\").withColumnRenamed(\"nationality\", \"driver_nationality\").withColumnRenamed(\"url\", \"driver_url\").withColumnRenamed(\"driverId\", \"driver_id\").withColumnRenamed(\"driverRef\", \"driver_ref\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['constructorId', 'constructorRef', 'name', 'nationality', 'url']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constructors.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "constructors = constructors.withColumnRenamed(\"name\", \"constructor_name\").withColumnRenamed(\"nationality\", \"constructor_nationality\").withColumnRenamed(\"url\", \"constructor_url\").withColumnRenamed(\"constructorId\", \"constructor_id\").withColumnRenamed(\"constructorRef\", \"constructor_ref\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['circuitId',\n",
       " 'circuitRef',\n",
       " 'name',\n",
       " 'location',\n",
       " 'country',\n",
       " 'lat',\n",
       " 'lng',\n",
       " 'alt',\n",
       " 'url']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "circuits.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "circuits = circuits.withColumnRenamed(\"circuitId\", \"circuit_id\").withColumnRenamed(\"circuitRef\", \"circuit_ref\").withColumnRenamed(\"name\", \"circuit_name\").withColumnRenamed(\"location\", \"circuit_location\").withColumnRenamed(\"country\", \"circuit_country\").withColumnRenamed(\"url\", \"circuit_url\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join DataFrames to create one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = races.join(results, \"race_id\", \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.join(qualifying, [\"race_id\", \"driver_id\", \"constructor_id\"], \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.join(drivers, \"driver_id\", \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3.join(constructors, \"constructor_id\", \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df4.join(circuits, \"circuit_id\", \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['circuit_id',\n",
       " 'constructor_id',\n",
       " 'driver_id',\n",
       " 'race_id',\n",
       " 'year',\n",
       " 'round',\n",
       " 'race_name',\n",
       " 'date',\n",
       " 'race_time',\n",
       " 'race_url',\n",
       " 'result_id',\n",
       " 'results_number',\n",
       " 'grid',\n",
       " 'result_position',\n",
       " 'positionText',\n",
       " 'positionOrder',\n",
       " 'points',\n",
       " 'laps',\n",
       " 'results_time',\n",
       " 'milliseconds',\n",
       " 'fastestLap',\n",
       " 'rank',\n",
       " 'fastestLapTime',\n",
       " 'fastestLapSpeed',\n",
       " 'status_id',\n",
       " 'qualifyId',\n",
       " 'qualifying_number',\n",
       " 'qualifying_position',\n",
       " 'q1',\n",
       " 'q2',\n",
       " 'q3',\n",
       " 'driver_ref',\n",
       " 'driver_number',\n",
       " 'code',\n",
       " 'forename',\n",
       " 'surname',\n",
       " 'dob',\n",
       " 'driver_nationality',\n",
       " 'driver_url',\n",
       " 'constructor_ref',\n",
       " 'constructor_name',\n",
       " 'constructor_nationality',\n",
       " 'constructor_url',\n",
       " 'circuit_ref',\n",
       " 'circuit_name',\n",
       " 'circuit_location',\n",
       " 'circuit_country',\n",
       " 'lat',\n",
       " 'lng',\n",
       " 'alt',\n",
       " 'circuit_url']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select necessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df5.select(['year', 'date', 'grid', 'status_id', 'qualifying_position', 'forename', 'surname', 'dob', 'driver_nationality', 'constructor_name', 'constructor_nationality', 'race_name', 'circuit_country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['year',\n",
       " 'date',\n",
       " 'grid',\n",
       " 'status_id',\n",
       " 'qualifying_position',\n",
       " 'forename',\n",
       " 'surname',\n",
       " 'dob',\n",
       " 'driver_nationality',\n",
       " 'constructor_name',\n",
       " 'constructor_nationality',\n",
       " 'race_name',\n",
       " 'circuit_country']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1 Grand Prix structure\n",
    "\n",
    "a F1 GP runs for 3 days in the weeekend and is made of 3 parts Practice session, Qualify scession and the actual Race.\n",
    "\n",
    "In the practice sesssion there are 3 stages FP1, FP2 and FP3 this is a free practice scession for teams to test their cars on Friday and saturday.\n",
    "Qualification session is also made of 3 stages Q1, Q2 and Q3 in this session all drivers compete to set the best lap time and bottom 5 drivers will be eliminated after Q1. top 15 drivers will participate in the Q2 and try to set best lap time and top 10 drivers will move to Q1 where they again set best best lap time they can and the cars position at the start of the race will be decided based on their qualifying time driver with best time will get to start at the front.\n",
    "Sunday scession is the Race and points will be awarded to top 10 drivers and top three will get to enjoy podium.\n",
    "This happens for a full season for a whole year at diffrent circuits and driver with the highest points will be awarded World championship and team with highest points will get Constructorschampionship(each team have two cars and two drivers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#considering data points from 2010\n",
    "data = data[data['year']>=2010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename the columns\n",
    "data = data.withColumnRenamed(\"race_name\", \"GP_name\").withColumnRenamed(\"circuit_country\", \"country\").withColumnRenamed(\"qualifying_position\", \"position\").withColumnRenamed(\"grid\", \"quali_pos\").withColumnRenamed(\"constructor_name\", \"constructor\").withColumn(\"date\", to_date(col(\"date\"))).withColumn(\"dob\", to_date(col(\"dob\"))).withColumn(\"driver\", concat(col(\"forename\"), lit(\" \"), col(\"surname\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating driver age parameter\n",
    "data = data.withColumn(\"age_at_gp_in_days\", datediff(col(\"date\"), col(\"dob\")))\n",
    "data = data.withColumn(\"age_at_gp_in_days\", expr(\"CAST(age_at_gp_in_days AS STRING)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn(\"constructor\", when(col(\"constructor\") == \"Force India\", \"Racing Point\")\n",
    "                                   .when(col(\"constructor\") == \"Sauber\", \"Alfa Romeo\")\n",
    "                                   .when(col(\"constructor\") == \"Lotus F1\", \"Renault\")\n",
    "                                   .when(col(\"constructor\") == \"Toro Rosso\", \"AlphaTauri\")\n",
    "                                   .otherwise(col(\"constructor\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn('driver_nationality', data['driver_nationality'].substr(1, 3))\n",
    "data = data.withColumn('constructor_nationality', data['constructor_nationality'].substr(1, 3))\n",
    "data = data.withColumn('country', when(data['country'] == 'UK', 'Bri').otherwise(data['country']))\n",
    "data = data.withColumn('country', when(data['country'] == 'USA', 'Ame').otherwise(data['country']))\n",
    "data = data.withColumn('country', when(data['country'] == 'Fra', 'Fre').otherwise(data['country']))\n",
    "data = data.withColumn('country', data['country'].substr(1, 3))\n",
    "data = data.withColumn('driver_home', (data['driver_nationality'] == data['country']).cast(\"int\"))\n",
    "data = data.withColumn('constructor_home', (data['constructor_nationality'] == data['country']).cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnf_statuses = [3, 4, 20, 29, 31, 41, 68, 73, 81, 97, 82, 104, 107, 130, 137]\n",
    "data = data.withColumn('driver_dnf', when(col('status_id').isin(dnf_statuses), 1).otherwise(0))\n",
    "data = data.withColumn('constructor_dnf', when(~col('status_id').isin(dnf_statuses + [1]), 1).otherwise(0))\n",
    "data = data.drop('forename', 'surname')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate DNF count by driver\n",
    "dnf_by_driver = data.groupBy('driver').agg({'driver_dnf': 'sum'})\n",
    "\n",
    "# Calculate race entered count by driver\n",
    "driver_race_entered = data.groupBy('driver').count()\n",
    "\n",
    "# Join the two calculated DataFrames\n",
    "driver_stats = dnf_by_driver.join(driver_race_entered, 'driver')\n",
    "\n",
    "# Calculate DNF ratio and driver confidence\n",
    "driver_stats = driver_stats.withColumn('driver_dnf_ratio', driver_stats['sum(driver_dnf)'] / driver_stats['count'])\n",
    "driver_stats = driver_stats.withColumn('driver_confidence', 1 - driver_stats['driver_dnf_ratio'])\n",
    "\n",
    "# Select necessary columns and convert to a Pandas DataFrame for creating the dictionary\n",
    "driver_confidence_dict = driver_stats.select('driver', 'driver_confidence').rdd.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate DNF count by constructor\n",
    "dnf_by_constructor = data.groupBy('constructor').agg({'constructor_dnf': 'sum'})\n",
    "\n",
    "# Calculate race entered count by constructor\n",
    "constructor_race_entered = data.groupBy('constructor').count()\n",
    "\n",
    "# Join the two calculated DataFrames\n",
    "constructor_stats = dnf_by_constructor.join(constructor_race_entered, 'constructor')\n",
    "\n",
    "# Calculate DNF ratio and constructor reliability\n",
    "constructor_stats = constructor_stats.withColumn('constructor_dnf_ratio', constructor_stats['sum(constructor_dnf)'] / constructor_stats['count'])\n",
    "constructor_stats = constructor_stats.withColumn('constructor_reliability', 1 - constructor_stats['constructor_dnf_ratio'])\n",
    "\n",
    "# Select necessary columns and convert to a Pandas DataFrame for creating the dictionary\n",
    "constructor_reliability_dict = constructor_stats.select('constructor', 'constructor_reliability').rdd.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for driver confidence and constructor reliability dictionaries\n",
    "driver_confidence_df = spark.createDataFrame(driver_confidence_dict.items(), [\"driver\", \"driver_confidence\"])\n",
    "constructor_reliability_df = spark.createDataFrame(constructor_reliability_dict.items(), [\"constructor\", \"constructor_reliability\"])\n",
    "\n",
    "# Adding 'driver_confidence' column\n",
    "data = data.join(driver_confidence_df, on='driver', how='left')\n",
    "\n",
    "# Adding 'constructor_reliability' column\n",
    "data = data.join(constructor_reliability_df, on='constructor', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists of active constructors and drivers\n",
    "active_constructors = ['Renault', 'Williams', 'McLaren', 'Ferrari', 'Mercedes',\n",
    "                       'AlphaTauri', 'Racing Point', 'Alfa Romeo', 'Red Bull',\n",
    "                       'Haas F1 Team']\n",
    "active_drivers = ['Daniel Ricciardo', 'Kevin Magnussen', 'Carlos Sainz',\n",
    "                  'Valtteri Bottas', 'Lance Stroll', 'George Russell',\n",
    "                  'Lando Norris', 'Sebastian Vettel', 'Kimi Räikkönen',\n",
    "                  'Charles Leclerc', 'Lewis Hamilton', 'Daniil Kvyat',\n",
    "                  'Max Verstappen', 'Pierre Gasly', 'Alexander Albon',\n",
    "                  'Sergio Pérez', 'Esteban Ocon', 'Antonio Giovinazzi',\n",
    "                  'Romain Grosjean', 'Nicholas Latifi']\n",
    "\n",
    "# Adding 'active_driver' column\n",
    "data = data.withColumn(\"active_driver\", when(col(\"driver\").isin(active_drivers), 1).otherwise(0))\n",
    "\n",
    "# Adding 'active_constructor' column\n",
    "data = data.withColumn(\"active_constructor\", when(col(\"constructor\").isin(active_constructors), 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['constructor',\n",
       " 'driver',\n",
       " 'year',\n",
       " 'date',\n",
       " 'quali_pos',\n",
       " 'status_id',\n",
       " 'position',\n",
       " 'dob',\n",
       " 'driver_nationality',\n",
       " 'constructor_nationality',\n",
       " 'GP_name',\n",
       " 'country',\n",
       " 'age_at_gp_in_days',\n",
       " 'driver_home',\n",
       " 'constructor_home',\n",
       " 'driver_dnf',\n",
       " 'constructor_dnf',\n",
       " 'driver_confidence',\n",
       " 'constructor_reliability',\n",
       " 'active_driver',\n",
       " 'active_constructor']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o407.csv.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 28.0 failed 1 times, most recent failure: Lost task 0.0 in stage 28.0 (TID 24) (cdac executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/sunbeam/.local/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 485, in main\n    (\"%d.%d\" % sys.version_info[:2], version))\nRuntimeError: Python in worker has different version 3.7 than that in driver 3.6, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:762)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:744)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2450)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2399)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2398)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2398)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1156)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1156)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1156)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2638)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2580)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2569)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/sunbeam/.local/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 485, in main\n    (\"%d.%d\" % sys.version_info[:2], version))\nRuntimeError: Python in worker has different version 3.7 than that in driver 3.6, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:762)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:744)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-c4830c6c90e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"/home/sunbeam/Desktop/f1/data/hnhr/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/sunbeam/.local/lib/python3.6/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m    953\u001b[0m                        \u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m                        encoding=encoding, emptyValue=emptyValue, lineSep=lineSep)\n\u001b[0;32m--> 955\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0morc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitionBy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sunbeam/.local/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sunbeam/.local/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sunbeam/.local/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o407.csv.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 28.0 failed 1 times, most recent failure: Lost task 0.0 in stage 28.0 (TID 24) (cdac executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/sunbeam/.local/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 485, in main\n    (\"%d.%d\" % sys.version_info[:2], version))\nRuntimeError: Python in worker has different version 3.7 than that in driver 3.6, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:762)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:744)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2450)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2399)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2398)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2398)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1156)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1156)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1156)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2638)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2580)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2569)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/sunbeam/.local/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 485, in main\n    (\"%d.%d\" % sys.version_info[:2], version))\nRuntimeError: Python in worker has different version 3.7 than that in driver 3.6, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:762)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:744)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "data.write.csv(r\"/home/sunbeam/Desktop/f1/data/hnhr/\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
